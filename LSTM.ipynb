{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Oy2lfMHxrKLe",
    "outputId": "6d92ddac-b231-4bb8-ecaf-9cfd68437d18"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "d284dc8506434a588be8d6f447a3abb2",
      "eb2e65a23e314adf98ce3ca29275f849",
      "025b018a42bc4561928846355dd94ba8",
      "ac03c782dfa54425945ac98af9e9124b",
      "ad483aca2bde495dbac08870e5ac7b65",
      "a945c9ac4607420f84a10c8b1c39dab6",
      "47a5151170d1417083a9843e5080ec93",
      "f84e2bf74b8a44a4949c26d9291e11fe",
      "b3638a8fa02847cf90921f190ab51907",
      "6d9ef47ece9a4983874dd33e82e3a5b2",
      "af7721c01ff644f9ac886193b84387b8",
      "9a6fa08c22c946df9eeea0764ffc1b01",
      "8c5b991c710e453dbc2649871c4c704c",
      "ac6b6b86f8604222b3ce429d14a12588",
      "7894ca67d7434272b0e1b714cb887b0d",
      "f31fba1d04154e1189e8d411e2a21cd6",
      "f246afdc81c349ce999c80792cfd9d48",
      "5398608101634e20838a3fe3f8f4c3ff",
      "ef46082830f64dd4896e285ae45f4e34",
      "1bde7b6d018940b4836a4706d39a73af",
      "e6e23e4895be43bb96fd38e7ba53f02a",
      "c43bd07176874978856707efc2d14d0a",
      "a38f26bbf57e4bb8addb387596549487",
      "f8f61f88b0c54acbb794e1d9cb57a48f",
      "d47a2e8a6be440b89f509960899d94d2",
      "20a0ae3e1b7744d49cef7e802c5875d8",
      "37d28d42fa9e4ae1b565d586f0cafb8f",
      "d17bf688ff5348d0b3af9679a49296e1",
      "38ab4812089f4a048c6c5543b619ddbc",
      "6b213d5e77784ce5be9a6baca016c33e",
      "55b2461496704dd9ad534cede0be936b",
      "02f1ee85a3774dffb90716a62532f2da",
      "a49844bf0bb847e1a16acd619122a487",
      "c0b22912d58a4af588034059da42b41e",
      "26b3743deb3649449b984d13fb5d2423",
      "283f6ca824fb41a78c7f0214a8b943a8",
      "0f8216989d574916b41cd7f4fce672cb",
      "e6e1df04b7334684a355cd740e034b2f",
      "c03ce7aba348448993dd84f9c26c1cd7",
      "14254bf9fe004431940ef7fb38426e2d",
      "de6ed697eb584c0b8c6906c071721281",
      "440a54e512e84a349cf45050f563ba9d",
      "b7f73ab633454b32927c679283517927",
      "cfd244d9f1e64c5a8666ab9d8c87adc0",
      "983aa68c67f64c46bcb792370f27a7ce",
      "73edf5b3909f4b96bd7c418f08b7529a",
      "300fec26301e473781b25eb7047b6711",
      "c33891b070d54e05a11f961d5755d10b",
      "1e0b595b2a064f88bd80d86c7e5ad884",
      "ebebfbead31942dba588b430100bd6f8",
      "1aa7d64c35c54e4fb0240c990f653d97",
      "0a174cafe0864f17ad3f18e345437610",
      "64b373d4769b48529cb0fd685ed03057",
      "807bc6e4828a41498636d241b97f7721",
      "60abd8e78ae341419981dad7def9d575",
      "e50d7e37b0bf41c2ae12584f00c38118",
      "d60ba92cc9314c69b0b26e19746f8a9b",
      "9146ecac68f7495fb6bc14011c5602d4",
      "f8cba0590f2b4513b46424ceea10348f",
      "39112c014afd422a81b71f0e89904c8d",
      "7cfc8be4a9b7473c8f5c6d3b16680497",
      "a9f3075bfdd6467493895b205f545e38",
      "6fd8322766c340f8acfb84c1e750b4d8",
      "de024d69ca8a4167817d24e22deef301",
      "dd5ef1a2f9ce454bb9cf9e363bd47a17",
      "d3998c4affcc47aba562185a45cfd5b2",
      "e12dd5f428e148029626ae42886c0d02",
      "181a0f614fb2440aa583dc01076e0e60",
      "3f7a9532b0b44414abec39769755bf76",
      "829c8fa2c3934391bb5c0f7c699365c8",
      "155d904fc00c4a8ea4800a5c33961dbb",
      "b680680cf4e54db6a62da48b36f9b48a",
      "31f8836119104d4b9f32c13d4558a583",
      "4ac37904d788428bb9868543fe2e5a9a",
      "2da642db35fa4c318786bc1bf0f0b853",
      "8c1a3e96abce4db0ace4f63c991dd79f",
      "56df026180034a0a990d4ca4ea10ca3b",
      "4449c090c37343738365dbc2b4cafba3",
      "483e7eb6e2a941fb96e36c6d4e165e6f",
      "b7cecab3ad044ba79eaec6d39c63a8f8",
      "767ea8fdf26749de9b062d51a5e4b92c",
      "f2032550f2ad473c9dc51806faa48983",
      "d54d63441a7e488b982c41162d580972",
      "1a6f420c6860417ea01d5b1a53929ca4",
      "09b264250ea24c6ebf8eaac836b4611b",
      "dd3545e588ba48128cfda65a3b83206a",
      "11dcc5ce4dcd4d9c893efeb267d130d3",
      "0a456dd6cade48af964dd66ffebf598d",
      "ae805e0db90b48d887142745f28c04b2",
      "259ca67b88da44689582f3b1863fb1f1",
      "a07ef2358f7c4d11946fc7a2c02a9fa1",
      "a01af08208324c8ea6318f32b937be13",
      "67a80fa3d28840e69661418930a2c013",
      "21e2e07dd54a4d3d99d26920b5ea01cc",
      "32f2d38546f847b2bf69afbc9fbae2d7",
      "e0d84bbc8d864a2ebe86b4b72f669562",
      "c66f297175624cf6a82bde5a8468d345",
      "30a28e0a35114c479ae35f817ad874b7",
      "a1ad9ebedad14e4ba3ae9e3c6eed07d9",
      "e32f710f84fd4b858afc53b614bbe012",
      "242946783a264e0d8e60ab5914b1fada",
      "97c8889fdaff43c2b09e7220c6db860f",
      "d62deb4bd8a342248e665bac3b33ce09",
      "f7edef11b65c466797f3962ce9f25beb",
      "140f65ac2ac143c99fbb795ee80dc7b7",
      "bcd74a295b3f4ec2af8002965820baff",
      "0a40c049e983418eb05d1d1eef19ea7d",
      "11628fafa6fe4ad19472ad68c3a041e4",
      "f3b0ba67a1e841cca284dcc1163fbc53",
      "d798dc53245e485eb46026b6b62b282d"
     ]
    },
    "id": "swtpl_Utss8u",
    "outputId": "c55ed532-771b-4424-883a-e94b4db2adc7"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datasets\n",
    "import pandas\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "\n",
    "# use the tokenizer from DistilRoBERT\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(examples):\n",
    "    \"\"\"Converts the text of each example to \"input_ids\", a sequence of integers\n",
    "    representing 1-hot vectors for each token in the text\"\"\"\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=64,\n",
    "                     padding=\"max_length\", return_tensors=\"tf\")\n",
    "\n",
    "def train(model_path=\"model\", train_path=\"/content/drive/MyDrive/Colab Notebooks/train.csv\", dev_path=\"/content/drive/MyDrive/Colab Notebooks/dev.csv\"):\n",
    "    # Load the CSVs into HuggingFace datasets to allow tokenizer usage\n",
    "    hf_dataset = datasets.load_dataset(\"csv\", data_files={\n",
    "        \"train\": train_path, \"validation\": dev_path\n",
    "    })\n",
    "\n",
    "    # Define labels as column names except the first (text column)\n",
    "    labels = hf_dataset[\"train\"].column_names[1:]\n",
    "\n",
    "    def gather_labels(example):\n",
    "        \"\"\"Convert label columns into a list of 0s and 1s\"\"\"\n",
    "        return {\"labels\": [float(example[l]) for l in labels]}\n",
    "\n",
    "    # Map the tokenizer and label gather functions\n",
    "    hf_dataset = hf_dataset.map(gather_labels)\n",
    "    hf_dataset = hf_dataset.map(tokenize, batched=True)\n",
    "\n",
    "    # Convert HuggingFace datasets to TensorFlow datasets\n",
    "    train_dataset = hf_dataset[\"train\"].to_tf_dataset(\n",
    "        columns=[\"input_ids\"], label_cols=\"labels\", batch_size=16, shuffle=True\n",
    "    )\n",
    "    dev_dataset = hf_dataset[\"validation\"].to_tf_dataset(\n",
    "        columns=[\"input_ids\"], label_cols=\"labels\", batch_size=16\n",
    "    )\n",
    "\n",
    "    # Define model architecture\n",
    "    input_layer = tf.keras.Input(shape=(64,), dtype=tf.int32, name=\"input_ids\")\n",
    "    embedding_layer = tf.keras.layers.Embedding(\n",
    "        input_dim=tokenizer.vocab_size, output_dim=128, input_length=64)(input_layer)\n",
    "    lstm_output = tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.3)\n",
    "    )(embedding_layer)\n",
    "    attention_output = tf.keras.layers.Attention()([lstm_output, lstm_output])\n",
    "    attention_output = tf.keras.layers.LayerNormalization()(attention_output) #trial\n",
    "    pooled_output = tf.keras.layers.GlobalAveragePooling1D()(attention_output)\n",
    "    dropout_layer = tf.keras.layers.Dropout(0.5)(pooled_output)\n",
    "    dense_layer = tf.keras.layers.Dense(256, activation=\"relu\")(dropout_layer) #trial # add L2 regularization for dense layer with value 0.0005\n",
    "    output_layer = tf.keras.layers.Dense(len(labels), activation=\"sigmoid\")(dense_layer)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1), #try label smmothing as 0.05\n",
    "        metrics=[tf.keras.metrics.F1Score(average=\"micro\", threshold=0.5)]\n",
    "    )\n",
    "\n",
    "    # model = transformers.TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(labels))\n",
    "    # optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "    # model.compile(optimizer=optimizer, loss=tf.keras.losses.BinaryCrossentropy(), metrics=[tf.keras.metrics.F1Score(average=\"micro\", threshold=0.5)])\n",
    "\n",
    "    #Define callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1\n",
    "    )\n",
    "    # try cosine or exponential LR for smoother rate decay\n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=3, verbose=1\n",
    "    )\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f\"{model_path}.keras\",\n",
    "        monitor=\"val_f1_score\",\n",
    "        mode=\"max\",\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=1)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=dev_dataset,\n",
    "        epochs=10,\n",
    "        callbacks=[early_stopping, lr_scheduler, model_checkpoint, tensorboard]\n",
    "    )\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225,
     "referenced_widgets": [
      "e22be94df18341fc889b5f759ceb54d5",
      "ce3248cc9f20498dadd54a6a8e7b914d",
      "a9772ede7b104b63805c24a231991573",
      "51ee0c73e7f0403da775cf72c629fb8c",
      "79e2a4d0de2c41878a23e8f03775cc96",
      "caed0022ffb04b17b11f3c017cb7112b",
      "edda2874046b4e30a954d63e9bcc88d5",
      "9e1d586e89c74b2485f66e5738d73e8d",
      "ac6e60f79565429bb1740fc23a6a73d3",
      "1201556c9451499e82d9cf08effc7124",
      "6f0c6115e3f14d4da194405a5a4c8f9d"
     ]
    },
    "id": "wnS1Dlk6CdHW",
    "outputId": "634e9552-42b3-444a-c05c-a0a45143b3cf"
   },
   "outputs": [],
   "source": [
    "def predict(model_path=\"/content/model\", input_path=\"/content/drive/MyDrive/Colab Notebooks/dev.csv\"):\n",
    "\n",
    "    # load the saved model\n",
    "    model = tf.keras.models.load_model(f\"{model_path}.keras\")\n",
    "\n",
    "    # load the data for prediction\n",
    "    # use Pandas here to make assigning labels easier later\n",
    "    df = pandas.read_csv(input_path)\n",
    "\n",
    "    # create input features in the same way as in train()\n",
    "    hf_dataset = datasets.Dataset.from_pandas(df)\n",
    "    hf_dataset = hf_dataset.map(tokenize, batched=True)\n",
    "    #hf_dataset = hf_dataset.map(to_bow)\n",
    "    tf_dataset = hf_dataset.to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "        batch_size=16)\n",
    "\n",
    "    # generate predictions from model\n",
    "    predictions = numpy.where(model.predict(tf_dataset) > 0.5, 1, 0)\n",
    "\n",
    "    # assign predictions to label columns in Pandas data frame\n",
    "    df.iloc[:, 1:] = predictions\n",
    "\n",
    "    # write the Pandas dataframe to a zipped CSV file\n",
    "    df.to_csv(\"submission_83.zip\", index=False, compression=dict(\n",
    "        method='zip', archive_name=f'submission_83.csv'))\n",
    "predict()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
