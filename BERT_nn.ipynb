{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6MAgW8lg6QF",
    "outputId": "e286389f-9f7d-4524-fde3-34a870cbdb4a"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZaGgXEBhTat"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555,
     "referenced_widgets": [
      "362591dff0784323b40124b31e1d01db",
      "5be3fe67cde449bb805692c76e2a4d19",
      "c064a1c2f812446ca265b630da27836a",
      "924b538f1a1b42c897e599834c608cef",
      "f409dccab28d42a097b405ccf36b97fc",
      "80ee19847e4e4653bcf6e3dba96fc2ba",
      "95e434c5885a4cc7bfb23baf7743f709",
      "ad686c2a0d01460a8b299d6e1276af61",
      "702f71cd76be4cb7852ba917d93f6eaa",
      "65e60d64d9884098a8e0e612bd94785e",
      "5e071b8deebd4707baca892cab3ac6ad"
     ]
    },
    "id": "8xP32IuF1do3",
    "outputId": "4f960f54-fda6-467b-daa3-84f1d563dfd9"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import datasets\n",
    "import pandas\n",
    "import transformers\n",
    "import tensorflow as tf\n",
    "import numpy\n",
    "\n",
    "# use the tokenizer from BERT\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize(examples):\n",
    "    \"\"\"Tokenize the input text and return input_ids and attention_mask.\"\"\"\n",
    "    tokens = tokenizer(examples[\"text\"], truncation=True, max_length=64, padding=\"max_length\")\n",
    "    return {\n",
    "        \"input_ids\": tokens[\"input_ids\"],\n",
    "        \"attention_mask\": tokens[\"attention_mask\"]\n",
    "    }\n",
    "\n",
    "def train(model_path=\"model\",\n",
    "          train_path=\"/content/drive/MyDrive/Colab Notebooks/train.csv\",\n",
    "          dev_path=\"/content/drive/MyDrive/Colab Notebooks/dev.csv\"):\n",
    "\n",
    "    # Load CSVs into HuggingFace datasets\n",
    "    hf_dataset = datasets.load_dataset(\"csv\", data_files={\n",
    "        \"train\": train_path,\n",
    "        \"validation\": dev_path\n",
    "    })\n",
    "\n",
    "    # Define label names\n",
    "    labels = hf_dataset[\"train\"].column_names[1:]  # First column is 'text'\n",
    "\n",
    "    def gather_labels(example):\n",
    "        \"\"\"Combine all label columns into a single list.\"\"\"\n",
    "        return {\"labels\": [float(example[l]) for l in labels]}\n",
    "\n",
    "    # Preprocess: label formatting + tokenization\n",
    "    hf_dataset = hf_dataset.map(gather_labels)\n",
    "    hf_dataset = hf_dataset.map(tokenize, batched=True)\n",
    "\n",
    "    # Convert HuggingFace dataset to TensorFlow dataset\n",
    "    train_dataset = hf_dataset[\"train\"].to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "        label_cols=\"labels\",\n",
    "        batch_size=16,\n",
    "        shuffle=True\n",
    "    )\n",
    "    dev_dataset = hf_dataset[\"validation\"].to_tf_dataset(\n",
    "        columns=[\"input_ids\", \"attention_mask\"],\n",
    "        label_cols=\"labels\",\n",
    "        batch_size=16\n",
    "    )\n",
    "\n",
    "    # Load pretrained BERT base model (without classification head)\n",
    "    bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "    # Input layers\n",
    "    input_ids = tf.keras.Input(shape=(64,), dtype=tf.int32, name=\"input_ids\")\n",
    "    attention_mask = tf.keras.Input(shape=(64,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "    # BERT output returns a tuple with multiple outputs (depending on the BERT variant you're using)\n",
    "    # output = (last_hidden_state, pooled_output, hidden_states, attentions)\n",
    "    # [0] gives last hidden state, This contains the contextualized embeddings for every token in the input\n",
    "    bert_output = bert_model(input_ids, attention_mask=attention_mask)[0]\n",
    "\n",
    "    # Pooling\n",
    "    pooled_output = tf.keras.layers.GlobalAveragePooling1D()(bert_output)\n",
    "    x = tf.keras.layers.LayerNormalization()(pooled_output)\n",
    "\n",
    "    # Dense head for multi-label classification\n",
    "    dropout = tf.keras.layers.Dropout(0.4)(x)\n",
    "    dense = tf.keras.layers.Dense(256, activation=\"relu\",\n",
    "                                   kernel_regularizer=tf.keras.regularizers.l2(0.0005))(dropout)\n",
    "    output = tf.keras.layers.Dense(len(labels), activation=\"sigmoid\")(dense)\n",
    "\n",
    "    # Final model\n",
    "    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "\n",
    "    # Compile\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.05),\n",
    "        metrics=[tf.keras.metrics.F1Score(average=\"micro\", threshold=0.5)]\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1\n",
    "    )\n",
    "    lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=2, verbose=1\n",
    "    )\n",
    "    model_checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=f\"{model_path}.keras\",\n",
    "        monitor=\"val_f1_score\", mode=\"max\", save_best_only=True, verbose=1\n",
    "    )\n",
    "    tensorboard = tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=1)\n",
    "\n",
    "    # Train\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=dev_dataset,\n",
    "        epochs=4,\n",
    "        callbacks=[early_stopping, lr_scheduler, model_checkpoint, tensorboard]\n",
    "    )\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
